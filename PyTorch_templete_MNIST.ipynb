{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "970737fa",
   "metadata": {},
   "source": [
    "# MNIST\n",
    "\n",
    "Follow this instruction:\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/07/how-to-train-an-image-classification-model-in-pytorch-and-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8351c1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2635d466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n"
     ]
    }
   ],
   "source": [
    "# version of pytorch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6c7a8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformations to be applied on images\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c71a885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the training and testing set\n",
    "trainset = datasets.MNIST('data/MNIST', download=True, train=True, transform=transform)\n",
    "testset = datasets.MNIST('data/MNIST', download=True, train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a13b543a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data/MNIST\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5,), std=(0.5,))\n",
       "           )"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50b09dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining trainloader and testloader\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e7bf04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# shape of training data\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85569dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7edda76700>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualizing the training images\n",
    "plt.imshow(images[0].numpy().squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78037f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# shape of validation data\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d0e380",
   "metadata": {},
   "source": [
    "# Defining Model Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60f11fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the model architecture\n",
    "class Net(nn.Module):   \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            # Defining a 2D convolution layer\n",
    "            nn.Conv2d(1, 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Defining another 2D convolution layer\n",
    "            nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            #nn.Linear(4 * 7 * 7, 10)\n",
    "            nn.Linear(4 * 7 * 7, 10)\n",
    "      )\n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ad398d",
   "metadata": {},
   "source": [
    "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "\n",
    "Parameters\n",
    "\n",
    "- in_channels (int) – Number of channels in the input image\n",
    "- out_channels (int) – Number of channels produced by the convolution\n",
    "- kernel_size (int or tuple) – Size of the convolving kernel\n",
    "- stride (int or tuple, optional) – Stride of the convolution. (Default: 1)\n",
    "- padding (int or tuple, optional) – Zero-padding added to both sides of the input (Default: 0)\n",
    "- padding_mode (string, optional) – zeros\n",
    "- dilation (int or tuple, optional) – Spacing between kernel elements. (Default: 1)\n",
    "- groups (int, optional) – Number of blocked connections from input to output channels. (Default: 1)\n",
    "- bias (bool, optional) – If True, adds a learnable bias to the output. (Default: True)\n",
    "\n",
    "BATCHNORM2D\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a4915ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (cnn_layers): Sequential(\n",
      "    (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linear_layers): Sequential(\n",
      "    (0): Linear(in_features=196, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# defining the model\n",
    "model = Net()\n",
    "# defining the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "# defining the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e1a901d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 0.19644056740743138\n",
      "Epoch 2 - Training loss: 0.09758875793469612\n",
      "Epoch 3 - Training loss: 0.08145711934471976\n",
      "Epoch 4 - Training loss: 0.07470306229399347\n",
      "Epoch 5 - Training loss: 0.07033862798439482\n",
      "Epoch 6 - Training loss: 0.06700670578256884\n",
      "Epoch 7 - Training loss: 0.06531948315799792\n",
      "Epoch 8 - Training loss: 0.06322327051855334\n",
      "Epoch 9 - Training loss: 0.06149278321863909\n",
      "Epoch 10 - Training loss: 0.057959989830504044\n",
      "CPU times: user 3min 2s, sys: 15.4 s, total: 3min 17s\n",
      "Wall time: 2min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(10):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        #This is where the model learns by backpropagating\n",
    "        loss.backward()\n",
    "        \n",
    "        #And optimizes its weights here\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(i+1, running_loss/len(trainloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94a2e96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Images Tested = 10000\n",
      "\n",
      "Model Accuracy = 0.9733\n"
     ]
    }
   ],
   "source": [
    "# getting predictions on test set and measuring the performance\n",
    "correct_count, all_count = 0, 0\n",
    "\n",
    "for images,labels in testloader:\n",
    "    for i in range(len(labels)):\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "        img = images[i].view(1, 1, 28, 28)\n",
    "        with torch.no_grad():\n",
    "            logps = model(img)\n",
    "\n",
    "    \n",
    "        ps = torch.exp(logps)\n",
    "        probab = list(ps.cpu()[0])\n",
    "        pred_label = probab.index(max(probab))\n",
    "        true_label = labels.cpu()[i]\n",
    "        if(true_label == pred_label):\n",
    "            correct_count += 1\n",
    "        all_count += 1\n",
    "\n",
    "print(\"Number Of Images Tested =\", all_count)\n",
    "print(\"\\nModel Accuracy =\", (correct_count/all_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0526fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
