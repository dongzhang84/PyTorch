{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae85dd5d",
   "metadata": {},
   "source": [
    "# Chapter 4: Transfer Learning And Other Tricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71f3a252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89742b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c860e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in transfer_model.named_parameters():\n",
    "    if(\"bn\" not in name):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "944a236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model.fc = nn.Sequential(nn.Linear(transfer_model.fc.in_features,500),\n",
    "nn.ReLU(),                                 \n",
    "nn.Dropout(), nn.Linear(500,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "244c2797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=20, device=\"cpu\"):\n",
    "    for epoch in range(epochs):\n",
    "        training_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.data.item() * inputs.size(0)\n",
    "        training_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        num_correct = 0 \n",
    "        num_examples = 0\n",
    "        for batch in val_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "            targets = targets.to(device)\n",
    "            loss = loss_fn(output,targets) \n",
    "            valid_loss += loss.data.item() * inputs.size(0)\n",
    "            correct = torch.eq(torch.max(F.softmax(output), dim=1)[1], targets).view(-1)\n",
    "            num_correct += torch.sum(correct).item()\n",
    "            num_examples += correct.shape[0]\n",
    "        valid_loss /= len(val_loader.dataset)\n",
    "\n",
    "        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, accuracy = {:.2f}'.format(epoch, training_loss,\n",
    "        valid_loss, num_correct / num_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e723f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_image(path):\n",
    "    try:\n",
    "        im = Image.open(path)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize((64,64)),    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225] )\n",
    "    ])\n",
    "train_data_path = \"data/train/\"\n",
    "train_data = torchvision.datasets.ImageFolder(root=train_data_path,transform=img_transforms, is_valid_file=check_image)\n",
    "val_data_path = \"data/val/\"\n",
    "val_data = torchvision.datasets.ImageFolder(root=val_data_path,transform=img_transforms, is_valid_file=check_image)\n",
    "batch_size=64\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_data_loader  = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") \n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0036f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n"
     ]
    }
   ],
   "source": [
    "print(len(val_data_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c286a4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model.to(device)\n",
    "optimizer = optim.Adam(transfer_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "390bb8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/var/folders/br/7q05nj8x3kl3x1g9947xjl700000gn/T/ipykernel_68579/2293702332.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  correct = torch.eq(torch.max(F.softmax(output), dim=1)[1], targets).view(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 0.50, Validation Loss: 0.37, accuracy = 0.83\n",
      "Epoch: 1, Training Loss: 0.17, Validation Loss: 0.26, accuracy = 0.88\n",
      "Epoch: 2, Training Loss: 0.07, Validation Loss: 0.19, accuracy = 0.90\n",
      "Epoch: 3, Training Loss: 0.03, Validation Loss: 0.19, accuracy = 0.92\n",
      "Epoch: 4, Training Loss: 0.03, Validation Loss: 0.35, accuracy = 0.87\n"
     ]
    }
   ],
   "source": [
    "train(transfer_model, optimizer,torch.nn.CrossEntropyLoss(), train_data_loader,val_data_loader, \n",
    "      epochs=5, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91fa1a9",
   "metadata": {},
   "source": [
    "## LR Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66899ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lr(model, loss_fn, optimizer, train_loader, init_value=1e-8, final_value=10.0, device=\"cpu\"):\n",
    "    number_in_epoch = len(train_loader) - 1\n",
    "    update_step = (final_value / init_value) ** (1 / number_in_epoch)\n",
    "    lr = init_value\n",
    "    optimizer.param_groups[0][\"lr\"] = lr\n",
    "    best_loss = 0.0\n",
    "    batch_num = 0\n",
    "    losses = []\n",
    "    log_lrs = []\n",
    "    for data in train_loader:\n",
    "        batch_num += 1\n",
    "        inputs, targets = data\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        # Crash out if loss explodes\n",
    "\n",
    "        if batch_num > 1 and loss > 4 * best_loss:\n",
    "            if(len(log_lrs) > 20):\n",
    "                return log_lrs[10:-5], losses[10:-5]\n",
    "            else:\n",
    "                return log_lrs, losses\n",
    "\n",
    "        # Record the best loss\n",
    "\n",
    "        if loss < best_loss or batch_num == 1:\n",
    "            best_loss = loss\n",
    "\n",
    "        # Store the values\n",
    "        losses.append(loss.item())\n",
    "        log_lrs.append((lr))\n",
    "\n",
    "        # Do the backward pass and optimize\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the lr for the next step and store\n",
    "\n",
    "        lr *= update_step\n",
    "        optimizer.param_groups[0][\"lr\"] = lr\n",
    "    if(len(log_lrs) > 20):\n",
    "        return log_lrs[10:-5], losses[10:-5]\n",
    "    else:\n",
    "        return log_lrs, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f619655b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEKCAYAAADEovgeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAUcklEQVR4nO3df5Bd5X3f8ffHksFjY8sObG2DgFUsuUSkjkk3NLVTxw7jIDxt5TRQL+M0pFbL0KB4ksY4MJMZE2ZSwtCaGTzgWh1kA+NaKJp2ZhMS09pyQuoS0KqWCdDI2UGkCDODDJgfwSMs8u0f92hyvb67e5fdR6tdvV8zd/ae5zznOd9HHO2Hc+7RPakqJElq6TVLXYAkaeUzbCRJzRk2kqTmDBtJUnOGjSSpOcNGktTc6qUu4Hh02mmn1ejo6FKXIUnLyt69e79TVSOD1hk2A4yOjjI5ObnUZUjSspLkr2da52U0SVJzho0kqTnDRpLUnGEjSWrOsJEkNWfYSJKaM2wkSc0ZNpKk5gwbSVJzho0kqTnDRpLUnGEjSWrOsJEkNWfYSJKaM2wkSc0ZNpKk5gwbSVJzho0kqTnDRpLUnGEjSWrOsJEkNWfYSJKaM2wkSc0ZNpKk5gwbSVJzho0kqTnDRpLUnGEjSWrOsJEkNWfYSJKaM2wkSc0ZNpKk5gwbSVJzho0kqbmmYZNkU5L9SaaSXD1g/clJ7urW359ktG/dNV37/iQXzjVmki8kOZBkX/d6d9d+TpL7khxO8omW85UkDba61cBJVgG3AB8EDgJ7kkxU1SN93bYAz1bV+iTjwA3AR5JsBMaBc4HTga8keWe3zWxjXlVVu6aV8gzwceDDiz5JSdJQWp7ZnA9MVdWjVfUysAPYPK3PZuD27v0u4IIk6dp3VNXhqjoATHXjDTPmD6iqp6pqD/D9xZqYJGl+WobNGcDjfcsHu7aBfarqCPAccOos28415u8meTDJTUlOnk+xSS5PMplk8tChQ/PZVJI0h5V0g8A1wDnATwE/AvzWfDauqm1VNVZVYyMjIy3qk6QTVsuweQI4s295bdc2sE+S1cAa4OlZtp1xzKp6snoOA5+nd8lNknQcaBk2e4ANSdYlOYneB/4T0/pMAJd17y8GdldVde3j3d1q64ANwAOzjZnk7d3P0LsZ4KGGc5MkzUOzu9Gq6kiSrcA9wCpge1U9nOQ6YLKqJoDbgDuTTNG7a2y82/bhJDuBR4AjwJVV9QrAoDG7XX4xyQgQYB9wRdf/bcAk8Cbgb5P8OrCxqp5vNXdJ0g9K70RC/cbGxmpycnKpy5CkZSXJ3qoaG7RuJd0gIEk6Thk2kqTmDBtJUnOGjSSpOcNGktScYSNJas6wkSQ1Z9hIkpozbCRJzRk2kqTmDBtJUnOGjSSpOcNGktScYSNJas6wkSQ1Z9hIkpozbCRJzRk2kqTmDBtJUnOGjSSpOcNGktScYSNJas6wkSQ1Z9hIkpozbCRJzRk2kqTmDBtJUnOGjSSpOcNGktScYSNJas6wkSQ11zRskmxKsj/JVJKrB6w/Ocld3fr7k4z2rbuma9+f5MK5xkzyhSQHkuzrXu/u2pPk5q7/g0l+suWcJUk/rFnYJFkF3AJcBGwELk2ycVq3LcCzVbUeuAm4odt2IzAOnAtsAm5NsmqIMa+qqnd3r31d20XAhu51OfDZRZ+sJGlWLc9szgemqurRqnoZ2AFsntZnM3B7934XcEGSdO07qupwVR0Aprrxhhlzus3AHdXz58Cbk7x9MSYoSRpOy7A5A3i8b/lg1zawT1UdAZ4DTp1l27nG/N3uUtlNSU6eRx0kuTzJZJLJQ4cODTdDSdJQVtINAtcA5wA/BfwI8Fvz2biqtlXVWFWNjYyMtKhPkk5YLcPmCeDMvuW1XdvAPklWA2uAp2fZdsYxq+rJ7lLZYeDz9C65DVuHJKmhlmGzB9iQZF2Sk+h94D8xrc8EcFn3/mJgd1VV1z7e3a22jt6H+w/MNubRz2G6z3w+DDzUt49f7u5K+2nguap6ssmMJUkDrW41cFUdSbIVuAdYBWyvqoeTXAdMVtUEcBtwZ5Ip4Bl64UHXbyfwCHAEuLKqXgEYNGa3yy8mGQEC7AOu6Nr/CPgQvZsMXgL+das5S5IGS+9EQv3GxsZqcnJyqcuQpGUlyd6qGhu0biXdICBJOk4ZNpKk5gwbSVJzho0kqTnDRpLUnGEjSWrOsJEkNWfYSJKaM2wkSc0ZNpKk5gwbSVJzho0kqTnDRpLUnGEjSWrOsJEkNTdU2CR5Q5LXdO/fmeSfJ3lt29IkSSvFsGc29wKvS3IG8D+AfwV8oVVRkqSVZdiwSVW9BPwL4NaqugQ4t11ZkqSVZOiwSfKPgY8Cd3dtq9qUJElaaYYNm18HrgH+e1U9nORHga81q0qStKKsHqZTVf0p8KcA3Y0C36mqj7csTJK0cgx7N9p/TfKmJG8AHgIeSXJV29IkSSvFsJfRNlbV88CHgT8G1tG7I02SpDkNGzav7f5dzYeBiar6PlDNqpIkrSjDhs3ngMeANwD3JjkbeL5VUZKklWXYGwRuBm7ua/rrJB9oU5IkaaUZ9gaBNUk+nWSye/0nemc5kiTNadjLaNuBF4B/2b2eBz7fqihJ0soy1GU04B1V9Yt9y7+TZF+DeiRJK9CwZzbfS/IzRxeSvBf4XpuSJEkrzbBnNlcAdyRZ0y0/C1zWpiRJ0koz1JlNVX2zqn4CeBfwrqo6D/i5ubZLsinJ/iRTSa4esP7kJHd16+9PMtq37pqufX+SC+cx5s1JXuxbPjvJV5M8mORPkqwdZs6SpMUzryd1VtXz3TcJAPz72fomWQXcAlwEbAQuTbJxWrctwLNVtR64Cbih23YjME7vMQabgFuTrJprzCRjwFum7eM/AndU1buA64Dr5zNnSdLCLeSx0Jlj/fnAVFU9WlUvAzuAzdP6bAZu797vAi5Ikq59R1UdrqoDwFQ33oxjdkF0I/DJafvYCOzu3n9tQA2SpMYWEjZzfV3NGcDjfcsHu7aBfarqCPAccOos28425lZ6X6Xz5LR9fJPeQ98AfgF4Y5JTpxeb5PKj/47o0KFDc0xNkjQfs4ZNkheSPD/g9QJw+jGqcU5JTgcuAT4zYPUngJ9N8g3gZ4EngFemd6qqbVU1VlVjIyMjTeuVpBPNrHejVdUbFzD2E8CZfctru7ZBfQ4mWQ2sAZ6eY9tB7ecB64Gp3lU4Xp9kqqrWV9W36c5skpwC/GJVfXcB85IkzdNCLqPNZQ+wIcm6JCfR+8B/YlqfCf7uFuqLgd1VVV37eHe32jpgA/DATGNW1d1V9baqGq2qUeCl7qYDkpzWPfANek8b3d5sxpKkgYb9dzbzVlVHkmwF7gFWAdu7R0pfB0xW1QRwG3BnkingGXrhQddvJ/AIcAS4sqpeARg05hylvB+4PkkB9wJXLvJUJUlzSO9EQv3GxsZqcnJyqcuQpGUlyd6qGhu0ruVlNEmSAMNGknQMGDaSpOYMG0lSc4aNJKk5w0aS1JxhI0lqzrCRJDVn2EiSmjNsJEnNGTaSpOYMG0lSc4aNJKk5w0aS1JxhI0lqzrCRJDVn2EiSmjNsJEnNGTaSpOYMG0lSc4aNJKk5w0aS1JxhI0lqzrCRJDVn2EiSmjNsJEnNGTaSpOYMG0lSc4aNJKk5w0aS1JxhI0lqrmnYJNmUZH+SqSRXD1h/cpK7uvX3JxntW3dN174/yYXzGPPmJC/2LZ+V5GtJvpHkwSQfajBVSdIsmoVNklXALcBFwEbg0iQbp3XbAjxbVeuBm4Abum03AuPAucAm4NYkq+YaM8kY8JZp+/htYGdVndeNeeuiTlSSNKeWZzbnA1NV9WhVvQzsADZP67MZuL17vwu4IEm69h1VdbiqDgBT3XgzjtkF0Y3AJ6fto4A3de/XAN9exDlKkobQMmzOAB7vWz7YtQ3sU1VHgOeAU2fZdrYxtwITVfXktH1cC/xSkoPAHwG/9uqmI0l6tVbEDQJJTgcuAT4zYPWlwBeqai3wIeDOJD807ySXJ5lMMnno0KG2BUvSCaZl2DwBnNm3vLZrG9gnyWp6l7menmXbmdrPA9YDU0keA16fZKrrswXYCVBV9wGvA06bXmxVbauqsaoaGxkZme9cJUmzaBk2e4ANSdYlOYneh/MT0/pMAJd17y8GdldVde3j3d1q64ANwAMzjVlVd1fV26pqtKpGgZe6mw4A/h9wAUCSH6MXNp66SNIxtLrVwFV1JMlW4B5gFbC9qh5Och0wWVUTwG30LmtNAc/QCw+6fjuBR4AjwJVV9QrAoDHnKOU3gf+S5Dfo3SzwK12gSZKOkfh794eNjY3V5OTkUpchSctKkr1VNTZo3Yq4QUCSdHwzbCRJzRk2kqTmDBtJUnOGjSSpOcNGktScYSNJas6wkSQ1Z9hIkpozbCRJzRk2kqTmDBtJUnOGjSSpOcNGktScYSNJas6wkSQ1Z9hIkpozbCRJzRk2kqTmDBtJUnOGjSSpOcNGktScYSNJas6wkSQ1Z9hIkpozbCRJzRk2kqTmDBtJUnOGjSSpOcNGktScYSNJaq5p2CTZlGR/kqkkVw9Yf3KSu7r19ycZ7Vt3Tde+P8mF8xjz5iQv9i3flGRf9/pWku8u/kwlSbNZ3WrgJKuAW4APAgeBPUkmquqRvm5bgGeran2SceAG4CNJNgLjwLnA6cBXkryz22bGMZOMAW/pr6OqfqOvpl8Dzlv82UqSZtPyzOZ8YKqqHq2ql4EdwOZpfTYDt3fvdwEXJEnXvqOqDlfVAWCqG2/GMbtwuxH45Cw1XQp8aVFmJ0kaWsuwOQN4vG/5YNc2sE9VHQGeA06dZdvZxtwKTFTVk4OKSXI2sA7Y/SrmIklagGaX0Y6lJKcDlwDvn6XbOLCrql6ZYYzLgcsBzjrrrMUuUZJOaC3PbJ4AzuxbXtu1DeyTZDWwBnh6lm1naj8PWA9MJXkMeH2SqWn7GmeWS2hVta2qxqpqbGRkZJj5SZKG1DJs9gAbkqxLchK9X/YT0/pMAJd17y8GdldVde3j3d1q64ANwAMzjVlVd1fV26pqtKpGgZeqav3RnSQ5h96NA/c1m60kaUbNLqNV1ZEkW4F7gFXA9qp6OMl1wGRVTQC3AXd2ZyHP0AsPun47gUeAI8CVRy9/DRpziHLG6d1wUIs7S0nSMOLv3x82NjZWk5OTS12GJC0rSfZW1digdX6DgCSpOcNGktScYSNJas6wkSQ1Z9hIkpozbCRJzRk2kqTmDBtJUnOGjSSpOcNGktScYSNJas6wkSQ15xdxDpDkEPBdek8OHWTNLOtOA77ToKzWZpvT8byvhYw1322H7T9Mv7n6rLRjzONr8fofz8fX2VU1+IFgVeVrwAvY9irXTS517Ys93+N5XwsZa77bDtt/mH5z9Vlpx5jH1+L1X67Hl5fRZvYHr3LdcnUs57SY+1rIWPPddtj+w/Sbq89KO8Y8vhav/7I8vryMtsiSTNYMz3OQFoPHmFpqdXx5ZrP4ti11AVrxPMbUUpPjyzMbSVJzntlIkpozbCRJzRk2kqTmVi91ASeSJBuBa4Gnga9W1a6lrUgrSZKzgJuBZ4BvVdXvLXFJWkGS/BPgo/RyY2NVvWc+23tmM6Qk25M8leShae2bkuxPMpXk6jmGuQj4TFX9O+CXmxWrZWeRjq9/AOyqqo8B5zUrVsvOYhxfVfVnVXUF8IfA7fOuwbvRhpPkfcCLwB1V9eNd2yrgW8AHgYPAHuBSYBVw/bQhPtb9/BTwEvCeqnrvMShdy8AiHV+vALuAAu6sqs8fm+p1vFuM46uqnuq22wlsqaoX5lODl9GGVFX3Jhmd1nw+MFVVjwIk2QFsrqrrgX86w1BXdv+R/1uzYrXsLMbxleQTwKe6sXYBho2Axfv91V2qfW6+QQNeRluoM4DH+5YPdm0DJRlNsg24A7ixcW1a/uZ1fAFfBj6e5D8DjzWsSyvDfI8vgC28yv+J8czmGKqqx4DLl7oOrUxV9RBw8VLXoZWrqj71arf1zGZhngDO7Fte27VJi8HjSy0d0+PLsFmYPcCGJOuSnASMAxNLXJNWDo8vtXRMjy/DZkhJvgTcB/z9JAeTbKmqI8BW4B7g/wI7q+rhpaxTy5PHl1o6Ho4vb32WJDXnmY0kqTnDRpLUnGEjSWrOsJEkNWfYSJKaM2wkSc0ZNtI8JHnxGO/vfx/j/b05ya8ey33qxGDYSEsoyazfTzjfB1Qtwj7fDBg2WnSGjbRASd6R5MtJ9ib5syTndO3/LMn9Sb6R5CtJ3tq1X5vkziRfB+7slrcn+ZMkjyb5eN/YL3Y/39+t35XkL5N8MUm6dR/q2vYmuTnJHw6o8VeSTCTZDXw1ySlJvprk/yT5iySbu66/B7wjyb4kN3bbXpVkT5IHk/xOyz9LrVx+67O0cNuAK6rqr5L8I+BW4OeA/wX8dFVVkn8DfBL4zW6bjcDPVNX3klwLnAN8AHgjsD/JZ6vq+9P2cx5wLvBt4OvAe5NMAp8D3ldVB7qvJZnJTwLvqqpnurObX6iq55OcBvx5kgngauDHq+rdAEl+HthA79knASaSvK+q7n21f1g6MRk20gIkOQV4D/D73YkGwMndz7XAXUneDpwEHOjbdKKqvte3fHdVHQYOJ3kKeCu954v0e6CqDnb73QeM0nv64qNVdXTsLzHzYyz+Z1U9c7R04D90T3D8W3rPMXnrgG1+vnt9o1s+hV74GDaaF8NGWpjXAN89eiYwzWeAT1fVRJL3A9f2rfubaX0P971/hcF/N4fpM5v+fX4UGAH+YVV9P8ljwOsGbBPg+qr63Dz3Jf0AP7ORFqCqngcOJLkEID0/0a1ew989H+SyRiXsB36075G/HxlyuzXAU13QfAA4u2t/gd6lvKPuAT7WncGR5Iwkf2/hZetE45mNND+vT9J/eevT9M4SPpvkt4HXAjuAb9I7k/n9JM8Cu4F1i11M95nPrwJfTvI39J5RMowvAn+Q5C+ASeAvu/GeTvL1JA8Bf1xVVyX5MeC+7jLhi8AvAU8t9ly0svmIAWmZS3JKVb3Y3Z12C/BXVXXTUtcl9fMymrT8/dvuhoGH6V0e8/MVHXc8s5EkNeeZjSSpOcNGktScYSNJas6wkSQ1Z9hIkpozbCRJzf1/iQH/ZNYT/GQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(lrs, losses) = find_lr(transfer_model, torch.nn.CrossEntropyLoss(),optimizer, train_data_loader,device=device)\n",
    "plt.plot(lrs, losses)\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b4f574",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
